import json
import shutil

from pathlib import Path
from typing import Iterable, List

from .common import download_to_file, extract_zip, PathLike


class MergeError(Exception):
    pass


def _gather_json_files(folder: Path) -> List[Path]:
    """Return a sorted list of .json files inside a folder (non-recursive)."""
    return sorted([p for p in folder.iterdir() if p.is_file() and p.suffix.lower() == ".json"])


def _iter_json_records_from_file(fp: Path):
    """
    Yield JSON-serializable objects from a file.
    If the file contains a JSON array, yield each element.
    If it contains a single object (dict, number, string...), yield it once.
    """
    text = fp.read_text(encoding="utf-8")
    obj = json.loads(text)
    if isinstance(obj, list):
        for item in obj:
            yield item
    else:
        yield obj


def _write_jsonl_atomic(records: Iterable, dest: Path) -> None:
    """
    Write records (iterable of JSON-serializable objects) to dest as JSON lines atomically.
    """
    tmp = dest.with_suffix(dest.suffix + ".part")
    with tmp.open("w", encoding="utf-8") as out:
        for rec in records:
            out.write(json.dumps(rec, ensure_ascii=False))
            out.write("\n")

    if not tmp.exists() or tmp.stat().st_size == 0:
        tmp.unlink(missing_ok=True)
        raise MergeError(f"Failed to write non-empty JSONL to {dest}")
    tmp.replace(dest)


def merge_eurlex_jsons_and_remove_dir(eurlex_dir: PathLike, prefix: str) -> List[Path]:
    eurlex_path = Path(eurlex_dir).resolve()
    if not eurlex_path.exists() or not eurlex_path.is_dir():
        raise MergeError(f"Provided eurlex_dir does not exist or is not a directory: {eurlex_path}")

    dataset_dir = eurlex_path / "dataset"
    splits = ["dev", "test", "train"]
    split_dirs = {s: dataset_dir / s for s in splits}

    # validate structure exists
    missing = [s for s, d in split_dirs.items() if not d.exists() or not d.is_dir()]
    if missing:
        raise MergeError(f"Missing expected split directories under {dataset_dir}: {missing}")

    out_parent = eurlex_path.parent  # parent directory where outputs will be written
    output_paths: List[Path] = []

    # For each split, collect records and write an atomic JSON lines file
    for split in splits:
        sd = split_dirs[split]
        json_files = _gather_json_files(sd)
        if not json_files:
            raise MergeError(f"No .json files found in {sd} â€” aborting to avoid producing empty output.")
        dest = out_parent / f"{prefix}_{split}.jsonl"

        # generator that yields records from all files in order
        def records_generator(files: List[Path]):
            for f in files:
                for rec in _iter_json_records_from_file(f):
                    yield rec

        _write_jsonl_atomic(records_generator(json_files), dest)
        output_paths.append(dest)

    # remove eurlex dir
    shutil.rmtree(eurlex_path, ignore_errors=True)

    return output_paths


# noinspection PyUnresolvedReferences,PyGlobalUndefined
def main(data_args) -> None:
    global logger, paths

    logger.info(f"Downloading {data_args.dataset_name}")

    download_dir = paths['download']['data']
    zip_file = download_to_file(data_args.dataset_url,  download_dir / 'eurlex.zip')
    extract_dir = download_dir / 'eurlex'
    extract_dir.mkdir(parents=True, exist_ok=True)
    logger.info(
        f"Extracting {zip_file} to {extract_dir}"
    )
    extract_zip(zip_file, extract_dir)
    merge_eurlex_jsons_and_remove_dir(extract_dir, data_args.dataset_name)
